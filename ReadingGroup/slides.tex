\documentclass{beamer}
%\documentclass[handout,t]{beamer}

\batchmode
% \usepackage{pgfpages}
% \pgfpagesuselayout{4 on 1}[letterpaper,landscape,border shrink=5mm]

\usepackage{amsmath,amssymb,amsfonts,enumerate,epsfig,bbm,calc,color,ifthen,capt-of}

\usetheme{Berlin}
\usecolortheme{mit}
\usefonttheme[onlymath]{serif}

\title{Task-agnostic Continual Learning with Hybrid Probabilistic Models}
\author[Polina Kirichenko \and Mehrdad Farajtabar]{
  Polina Kirichenko \inst{1} \and Mehrdad Farajtabar \inst{2} \and Dushyant Rao \inst{2} \\
  Balaji Lakshminarayanan \inst{3} \and Nir Levine \inst{2} \and Ang Li \inst{2} \\
  Huiyi Hu \inst{2} \and Andrew Gordon Wilson \inst{1} \and Razvan Pascanu \inst{2}
}

\institute[NYU \and DeepMind \and Google Brain]{
  \inst{1} New York University \and
  \inst{2} DeepMind \and
  \inst{3} Google Brain
}
\date{\today}
% \pgfdeclareimage[height=0.5cm]{mit-logo}{mit-logo.pdf}
% \logo{\pgfuseimage{mit-logo}\hspace*{0.3cm}}

% \AtBeginSection[]
% {
%   \begin{frame}<beamer>
%     \frametitle{Outline}
%     \tableofcontents[currentsection]
%   \end{frame}
% }
\beamerdefaultoverlayspecification{<+->}
% -----------------------------------------------------------------------------
\begin{document}
% -----------------------------------------------------------------------------

\frame{\titlepage}

\section[Outline]{}
\begin{frame}{Outline}
  \tableofcontents
\end{frame}

% -----------------------------------------------------------------------------
\section{Introduction}
% \subsection{Smoot's Ear: The Measure of Humanity}
\begin{frame}{Existing Approaches}
  \begin{itemize}
    \item<1-> re-sample the data or design specific loss functions that better facilitate learning with imbalanced data
    \item<1-> enhance recognition performance of the tail classes by transferring knowledge from the head classes
  \end{itemize}
  
  \begin{block}{Title of}
    hi
  \end{block}
\end{frame}

\begin{frame}{Our Contribution}
  \begin{itemize}
    \item <1-> Hybrid Continual Learning (HCL) - a normalizing flow-based approach.
    \item Generative replay and a novel functional regularization are employed to alleviate forgetting. The functional regularization is shown to be better than generalize replay.
    \item <1-> HCL achieves strong performance on \emph{split MNIST}, \emph{split CIFAR}, \emph{SVHN-MNIST} and \emph{MNIST-SVHN} datasets.
    \item <1-> HCL can detect task boundaries and identify new as well as recurring tasks.
  \end{itemize}
\end{frame}

% -----------------------------------------------------------------------------
\section{Background and Notation}
\begin{frame}{Continual Learning (CL)}
  \begin{itemize}
    \item <1-> A CL model $g_\theta : \mathcal{X} \rightarrow \mathcal{Y}$.
    \item <1-> A sequence of $\tau$ supervised tasks: $T_{t_1}, \ T_{t_2}, \dots, T_{t_\tau}$. $\tau$ is not known in advance.
    \item <1-> Each task $T_i = \{ (x_j^i, y_j^i) \}_{j = 1}^{N_i}$, where $x_j^i \in \mathcal{X}^i$ and $y_j^i \in \mathcal{Y}^i$.
    \item <1-> The corresponding data distribution of task $T_i$ is $p_i(x, y)$.
    \item <1-> \textbf{Constraint}: While training on a task $T_i$ the model cannot access to the data from previous $T_1, \dots, T_{i-1}$ or future tasks $T_{i+1}, \dots, T_{\tau}$.
    \item <1-> \textbf{Objective}: Minimize $\sum_{i = 1}^M \mathbb{E}_{x, y \sim p_i(\cdot, \cdot)}l(g_\theta(x), y)$ for some risk function $l(\cdot, \cdot)$ and generalize well on all tasks after training.
  \end{itemize} 
\end{frame}

% -----------------------------------------------------------------------------
\section{HCL}
\begin{frame}{Modeling the Data Distribution}
  \begin{itemize}
    \item <1-> $p_t(x, y)$: the joint distribution of the data $x$ and the class label $y$ conditioned on a task $t$.
    \[
      p_t(x, y) \approx \hat{p}(x, y | t) = \hat{p}_X(x | y, t) \hat{p}(y | t)
    \]
    \item <1-> $\hat{p}_X(x | y, t)$ is modeled by a normalizing flow $f_\theta$ with a base distribution $\hat{p}_Z = \mathcal{N}(\mu_{y, t}, I)$.
    \[
      \hat{p}_X(x | y, t) = f_\theta^{-1}\left( \mathcal{N}(\mu_{y, t}, I) \right)
    \]
    \item <1-> $\mu_{y, t}$ is the mean of the latent distribution corresponding to the class $y$ and task $t$.
    \item <1-> $\hat{p}(y | t)$ is assumed to be a uniform distribution over the classes for each task: $\hat{p}(y | t) = 1/K$.
  \end{itemize}
\end{frame}

\begin{frame}{Task Identification}
  \begin{itemize}
    \item <1-> log-likelihood
    \[
      S_1(B, t) = \sum_{(x_j, y_j) \in B} \hat{p}_X(x_j | y_j, t)
    \]
    \item <1-> log-likelihood of the latent variable 
    \[
      S_2(B, t) = \sum_{(x_j, y_j) \in B} \hat{p}_Z(f_\theta(x_j) | y_j, t)
    \]
    \item <1-> log-determinant of the Jacobian
    \[
      S_3(B, t) = S_1(B, t) - S_2(B, t)
    \]
  \end{itemize}
\end{frame}

\begin{frame}{Generative Replay}

\end{frame}

\begin{frame}{Functional Regularization}
  
\end{frame}

\begin{frame}{Theoretical Analysis}
  
\end{frame}

% -----------------------------------------------------------------------------
\section{Experiments}

% -----------------------------------------------------------------------------
\section{Discussion}
\begin{frame}
  
\end{frame}

% -----------------------------------------------------------------------------
\end{document}
